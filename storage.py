"""\nData storage and persistence layer for InfoFlow MCP Server.\nSupports multiple storage backends: ChromaDB, Redis, SQLite\n"""\n\nimport json\nimport sqlite3\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\nfrom loguru import logger\n\ntry:\n    import chromadb\n    from chromadb.config import Settings\n    CHROMADB_AVAILABLE = True\nexcept ImportError:\n    CHROMADB_AVAILABLE = False\n    logger.warning(\"ChromaDB not available. Install with: pip install chromadb\")\n\ntry:\n    import redis\n    REDIS_AVAILABLE = True\nexcept ImportError:\n    REDIS_AVAILABLE = False\n    logger.warning(\"Redis not available. Install with: pip install redis\")\n\nfrom models import ContentItem, FilteredResult\nfrom config import StorageConfig\n\n\nclass StorageBackend(ABC):\n    \"\"\"Abstract base class for storage backends.\"\"\"\n    \n    @abstractmethod\n    async def store_item(self, item: ContentItem) -> bool:\n        \"\"\"Store a content item.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def get_item(self, item_id: str) -> Optional[ContentItem]:\n        \"\"\"Retrieve a content item by ID.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def search_items(\n        self, \n        query: str, \n        limit: int = 10,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[ContentItem]:\n        \"\"\"Search for content items.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def delete_old_items(self, days: int) -> int:\n        \"\"\"Delete items older than specified days.\"\"\"\n        pass\n    \n    @abstractmethod\n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        pass\n\n\nclass ChromaDBStorage(StorageBackend):\n    \"\"\"ChromaDB vector storage implementation.\"\"\"\n    \n    def __init__(self, config: StorageConfig):\n        if not CHROMADB_AVAILABLE:\n            raise RuntimeError(\"ChromaDB is not installed\")\n        \n        self.config = config\n        storage_path = Path(config.path or \"./data/chromadb\")\n        storage_path.mkdir(parents=True, exist_ok=True)\n        \n        self.client = chromadb.PersistentClient(\n            path=str(storage_path),\n            settings=Settings(anonymized_telemetry=False)\n        )\n        \n        self.collection = self.client.get_or_create_collection(\n            name=\"infoflow_content\",\n            metadata={\"description\": \"InfoFlow content storage\"}\n        )\n        \n        logger.info(f\"ChromaDB initialized at {storage_path}\")\n    \n    async def store_item(self, item: ContentItem) -> bool:\n        \"\"\"Store a content item in ChromaDB.\"\"\"\n        try:\n            # Combine title and content for embedding\n            text = f\"{item.title}\\n\\n{item.content}\"\n            \n            metadata = {\n                \"item_id\": item.id,\n                \"title\": item.title,\n                \"url\": item.url,\n                \"source\": item.source,\n                \"content_type\": item.content_type,\n                \"published_date\": item.published_date.isoformat() if item.published_date else None,\n                \"relevance_score\": item.relevance_score or 0.0,\n                \"quality_score\": item.quality_score or 0.0,\n                \"author\": item.author or \"\",\n                \"tags\": json.dumps(item.tags),\n            }\n            \n            self.collection.upsert(\n                ids=[item.id],\n                documents=[text],\n                metadatas=[metadata]\n            )\n            \n            logger.debug(f\"Stored item {item.id} in ChromaDB\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error storing item in ChromaDB: {e}\")\n            return False\n    \n    async def get_item(self, item_id: str) -> Optional[ContentItem]:\n        \"\"\"Retrieve a content item by ID.\"\"\"\n        try:\n            result = self.collection.get(ids=[item_id])\n            \n            if not result['ids']:\n                return None\n            \n            metadata = result['metadatas'][0]\n            document = result['documents'][0]\n            \n            # Parse document back to title and content\n            parts = document.split('\\n\\n', 1)\n            title = parts[0]\n            content = parts[1] if len(parts) > 1 else \"\"\n            \n            return ContentItem(\n                id=metadata['item_id'],\n                title=title,\n                content=content,\n                url=metadata['url'],\n                source=metadata['source'],\n                content_type=metadata['content_type'],\n                published_date=datetime.fromisoformat(metadata['published_date']) if metadata['published_date'] else None,\n                relevance_score=metadata.get('relevance_score'),\n                quality_score=metadata.get('quality_score'),\n                author=metadata.get('author'),\n                tags=json.loads(metadata.get('tags', '[]'))\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error retrieving item from ChromaDB: {e}\")\n            return None\n    \n    async def search_items(\n        self,\n        query: str,\n        limit: int = 10,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[ContentItem]:\n        \"\"\"Search for content items using semantic search.\"\"\"\n        try:\n            where_filter = {}\n            if filters:\n                if 'source' in filters:\n                    where_filter['source'] = filters['source']\n                if 'content_type' in filters:\n                    where_filter['content_type'] = filters['content_type']\n            \n            results = self.collection.query(\n                query_texts=[query],\n                n_results=limit,\n                where=where_filter if where_filter else None\n            )\n            \n            items = []\n            if results['ids'] and results['ids'][0]:\n                for i, item_id in enumerate(results['ids'][0]):\n                    metadata = results['metadatas'][0][i]\n                    document = results['documents'][0][i]\n                    \n                    parts = document.split('\\n\\n', 1)\n                    title = parts[0]\n                    content = parts[1] if len(parts) > 1 else \"\"\n                    \n                    items.append(ContentItem(\n                        id=metadata['item_id'],\n                        title=title,\n                        content=content,\n                        url=metadata['url'],\n                        source=metadata['source'],\n                        content_type=metadata['content_type'],\n                        published_date=datetime.fromisoformat(metadata['published_date']) if metadata['published_date'] else None,\n                        relevance_score=metadata.get('relevance_score'),\n                        quality_score=metadata.get('quality_score'),\n                        author=metadata.get('author'),\n                        tags=json.loads(metadata.get('tags', '[]'))\n                    ))\n            \n            return items\n            \n        except Exception as e:\n            logger.error(f\"Error searching items in ChromaDB: {e}\")\n            return []\n    \n    async def delete_old_items(self, days: int) -> int:\n        \"\"\"Delete items older than specified days.\"\"\"\n        try:\n            cutoff_date = datetime.now() - timedelta(days=days)\n            # ChromaDB doesn't have direct date filtering, so we need to get all and filter\n            all_items = self.collection.get()\n            \n            ids_to_delete = []\n            for i, metadata in enumerate(all_items['metadatas']):\n                if metadata.get('published_date'):\n                    pub_date = datetime.fromisoformat(metadata['published_date'])\n                    if pub_date < cutoff_date:\n                        ids_to_delete.append(all_items['ids'][i])\n            \n            if ids_to_delete:\n                self.collection.delete(ids=ids_to_delete)\n                logger.info(f\"Deleted {len(ids_to_delete)} old items\")\n            \n            return len(ids_to_delete)\n            \n        except Exception as e:\n            logger.error(f\"Error deleting old items: {e}\")\n            return 0\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        try:\n            count = self.collection.count()\n            return {\n                \"backend\": \"chromadb\",\n                \"total_items\": count,\n                \"path\": str(self.config.path)\n            }\n        except Exception as e:\n            logger.error(f\"Error getting stats: {e}\")\n            return {\"error\": str(e)}\n\n\nclass SQLiteStorage(StorageBackend):\n    \"\"\"SQLite storage implementation for simpler deployments.\"\"\"\n    \n    def __init__(self, config: StorageConfig):\n        self.config = config\n        db_path = Path(config.sqlite_path or \"./data/infoflow.db\")\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        self.db_path = str(db_path)\n        self._init_database()\n        logger.info(f\"SQLite initialized at {db_path}\")\n    \n    def _init_database(self):\n        \"\"\"Initialize database schema.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n        \n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS content_items (\n                id TEXT PRIMARY KEY,\n                title TEXT NOT NULL,\n                content TEXT,\n                url TEXT,\n                source TEXT,\n                content_type TEXT,\n                published_date TEXT,\n                relevance_score REAL,\n                quality_score REAL,\n                author TEXT,\n                tags TEXT,\n                created_at TEXT DEFAULT CURRENT_TIMESTAMP\n            )\n        ''')\n        \n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_published_date \n            ON content_items(published_date)\n        ''')\n        \n        cursor.execute('''\n            CREATE INDEX IF NOT EXISTS idx_source \n            ON content_items(source)\n        ''')\n        \n        conn.commit()\n        conn.close()\n    \n    async def store_item(self, item: ContentItem) -> bool:\n        \"\"\"Store a content item in SQLite.\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute('''\n                INSERT OR REPLACE INTO content_items \n                (id, title, content, url, source, content_type, published_date,\n                 relevance_score, quality_score, author, tags)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (\n                item.id,\n                item.title,\n                item.content,\n                item.url,\n                item.source,\n                item.content_type,\n                item.published_date.isoformat() if item.published_date else None,\n                item.relevance_score,\n                item.quality_score,\n                item.author,\n                json.dumps(item.tags)\n            ))\n            \n            conn.commit()\n            conn.close()\n            \n            logger.debug(f\"Stored item {item.id} in SQLite\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Error storing item in SQLite: {e}\")\n            return False\n    \n    async def get_item(self, item_id: str) -> Optional[ContentItem]:\n        \"\"\"Retrieve a content item by ID.\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            conn.row_factory = sqlite3.Row\n            cursor = conn.cursor()\n            \n            cursor.execute('SELECT * FROM content_items WHERE id = ?', (item_id,))\n            row = cursor.fetchone()\n            conn.close()\n            \n            if not row:\n                return None\n            \n            return ContentItem(\n                id=row['id'],\n                title=row['title'],\n                content=row['content'],\n                url=row['url'],\n                source=row['source'],\n                content_type=row['content_type'],\n                published_date=datetime.fromisoformat(row['published_date']) if row['published_date'] else None,\n                relevance_score=row['relevance_score'],\n                quality_score=row['quality_score'],\n                author=row['author'],\n                tags=json.loads(row['tags']) if row['tags'] else []\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error retrieving item from SQLite: {e}\")\n            return None\n    \n    async def search_items(\n        self,\n        query: str,\n        limit: int = 10,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[ContentItem]:\n        \"\"\"Search for content items using full-text search.\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            conn.row_factory = sqlite3.Row\n            cursor = conn.cursor()\n            \n            sql = 'SELECT * FROM content_items WHERE (title LIKE ? OR content LIKE ?)'\n            params = [f'%{query}%', f'%{query}%']\n            \n            if filters:\n                if 'source' in filters:\n                    sql += ' AND source = ?'\n                    params.append(filters['source'])\n                if 'content_type' in filters:\n                    sql += ' AND content_type = ?'\n                    params.append(filters['content_type'])\n            \n            sql += ' ORDER BY published_date DESC LIMIT ?'\n            params.append(limit)\n            \n            cursor.execute(sql, params)\n            rows = cursor.fetchall()\n            conn.close()\n            \n            items = []\n            for row in rows:\n                items.append(ContentItem(\n                    id=row['id'],\n                    title=row['title'],\n                    content=row['content'],\n                    url=row['url'],\n                    source=row['source'],\n                    content_type=row['content_type'],\n                    published_date=datetime.fromisoformat(row['published_date']) if row['published_date'] else None,\n                    relevance_score=row['relevance_score'],\n                    quality_score=row['quality_score'],\n                    author=row['author'],\n                    tags=json.loads(row['tags']) if row['tags'] else []\n                ))\n            \n            return items\n            \n        except Exception as e:\n            logger.error(f\"Error searching items in SQLite: {e}\")\n            return []\n    \n    async def delete_old_items(self, days: int) -> int:\n        \"\"\"Delete items older than specified days.\"\"\"\n        try:\n            cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()\n            \n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute(\n                'DELETE FROM content_items WHERE published_date < ?',\n                (cutoff_date,)\n            )\n            \n            deleted_count = cursor.rowcount\n            conn.commit()\n            conn.close()\n            \n            logger.info(f\"Deleted {deleted_count} old items\")\n            return deleted_count\n            \n        except Exception as e:\n            logger.error(f\"Error deleting old items: {e}\")\n            return 0\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        try:\n            conn = sqlite3.connect(self.db_path)\n            cursor = conn.cursor()\n            \n            cursor.execute('SELECT COUNT(*) FROM content_items')\n            total = cursor.fetchone()[0]\n            \n            cursor.execute('SELECT AVG(relevance_score) FROM content_items')\n            avg_relevance = cursor.fetchone()[0] or 0.0\n            \n            conn.close()\n            \n            return {\n                \"backend\": \"sqlite\",\n                \"total_items\": total,\n                \"avg_relevance_score\": round(avg_relevance, 2),\n                \"path\": self.db_path\n            }\n        except Exception as e:\n            logger.error(f\"Error getting stats: {e}\")\n            return {\"error\": str(e)}\n\n\nclass StorageManager:\n    \"\"\"Main storage manager that routes to appropriate backend.\"\"\"\n    \n    def __init__(self, config: StorageConfig):\n        self.config = config\n        self.backend = self._initialize_backend()\n    \n    def _initialize_backend(self) -> StorageBackend:\n        \"\"\"Initialize the appropriate storage backend.\"\"\"\n        backend_type = self.config.type.lower()\n        \n        if backend_type == \"chromadb\":\n            if not CHROMADB_AVAILABLE:\n                logger.warning(\"ChromaDB not available, falling back to SQLite\")\n                return SQLiteStorage(self.config)\n            return ChromaDBStorage(self.config)\n        \n        elif backend_type == \"sqlite\":\n            return SQLiteStorage(self.config)\n        \n        else:\n            logger.warning(f\"Unknown storage type: {backend_type}, using SQLite\")\n            return SQLiteStorage(self.config)\n    \n    async def store_item(self, item: ContentItem) -> bool:\n        \"\"\"Store a content item.\"\"\"\n        return await self.backend.store_item(item)\n    \n    async def get_item(self, item_id: str) -> Optional[ContentItem]:\n        \"\"\"Retrieve a content item by ID.\"\"\"\n        return await self.backend.get_item(item_id)\n    \n    async def search_items(\n        self,\n        query: str,\n        limit: int = 10,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[ContentItem]:\n        \"\"\"Search for content items.\"\"\"\n        return await self.backend.search_items(query, limit, filters)\n    \n    async def delete_old_items(self, days: int) -> int:\n        \"\"\"Delete items older than specified days.\"\"\"\n        return await self.backend.delete_old_items(days)\n    \n    async def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get storage statistics.\"\"\"\n        return await self.backend.get_stats()\n    \n    async def store_filtered_results(self, results: FilteredResult) -> bool:\n        \"\"\"Store filtered results (stores all items in the result).\"\"\"\n        success_count = 0\n        for item in results.filtered_items:\n            if await self.store_item(item):\n                success_count += 1\n        \n        logger.info(f\"Stored {success_count}/{len(results.filtered_items)} filtered items\")\n        return success_count == len(results.filtered_items)\n